# -*- coding: utf-8 -*-
"""ASSIGNMENT_2_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16afU5TFsUYuKu2jinn1ke0pGZZXKPSR8
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn import clone
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import Ridge, Lasso, LassoLarsIC
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split, ParameterGrid
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.utils import check_random_state, shuffle
from sklearn.datasets import load_iris
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.linear_model import LinearRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import mean_squared_error
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

"""# **Part 1**"""

data = {
    'Color': ['Red', 'Red', 'Red', 'Yellow', 'Yellow', 'Blue', 'Yellow', 'Yellow', 'Red', 'Blue', 'Red', 'Red', 'Blue', 'Red'],
    'Type': ['Sports', 'Sports', 'Sports', 'Sports', 'Sports', 'SUV', 'SUV', 'SUV', 'SUV', 'Sports', 'SUV', 'SUV', 'Sports', 'SUV'],
    'Origin': ['Domestic', 'Domestic', 'Domestic', 'Domestic', 'Imported', 'Imported', 'Imported', 'Domestic', 'Imported', 'Imported', 'Domestic', 'Domestic', 'Imported', 'Imported'],
    'Stolen': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes']
}
df = pd.DataFrame(data)

def calc_conditional_prob(feature, value, target_value):
    count = len(df[(df[feature] == value) & (df['Stolen'] == target_value)])
    total_count = len(df[df['Stolen'] == target_value])
    unique_values = df[feature].nunique()
    return (count + 1) / (total_count + unique_values)

def calc_laplace_conditional_prob(feature, value, target_value):
    count = len(df[(df[feature] == value) & (df['Stolen'] == target_value)])
    total_count = len(df[df['Stolen'] == target_value])
    return (count ) / (total_count )

class_counts = df['Stolen'].value_counts()
total_samples = df.shape[0]
p_yes = class_counts['Yes'] / total_samples
p_no = class_counts['No'] / total_samples
print (class_counts)
print (total_samples)

print (p_yes)
print (p_no)

def calc_laplace_new_inst(color,typee,origin):
  color_prob_yes = calc_laplace_conditional_prob('Color', color, 'Yes')
  type_prob_yes = calc_laplace_conditional_prob('Type', typee, 'Yes')
  origin_prob_yes = calc_laplace_conditional_prob('Origin',origin, 'Yes')
  color_prob_no = calc_laplace_conditional_prob('Color',color, 'No')
  type_prob_no = calc_laplace_conditional_prob('Type', typee, 'No')
  origin_prob_no = calc_laplace_conditional_prob('Origin', origin, 'No')
  color_counts = df['Color'].value_counts()[color]
  type_counts = df['Type'].value_counts()[typee]
  origin_counts = df['Origin'].value_counts()[origin]
  posterior_prob_yes = p_yes * color_prob_yes * type_prob_yes * origin_prob_yes
  posterior_prob_no = p_no * color_prob_no * type_prob_no * origin_prob_no
  #norm= ((origin_counts)/df.shape[0])*((type_counts)/df.shape[0])*((color_counts)/df.shape[0])
  norm= posterior_prob_no+ posterior_prob_yes
  return (posterior_prob_yes/norm),(posterior_prob_no/norm)

def calc_new_inst(color,typee,origin):
  color_prob_yes = calc_conditional_prob('Color', color, 'Yes')
  type_prob_yes = calc_conditional_prob('Type', typee, 'Yes')
  origin_prob_yes = calc_conditional_prob('Origin',origin, 'Yes')
  color_prob_no = calc_conditional_prob('Color',color, 'No')
  type_prob_no = calc_conditional_prob('Type', typee, 'No')
  origin_prob_no = calc_conditional_prob('Origin', origin, 'No')
  color_counts = df['Color'].value_counts()[color]
  type_counts = df['Type'].value_counts()[typee]
  origin_counts = df['Origin'].value_counts()[origin]
  posterior_prob_yes = p_yes * color_prob_yes * type_prob_yes * origin_prob_yes
  posterior_prob_no = p_no * color_prob_no * type_prob_no * origin_prob_no
  #norm= ((origin_counts)/df.shape[0])*((type_counts)/df.shape[0])*((color_counts)/df.shape[0])
  norm= posterior_prob_no+ posterior_prob_yes
  return (posterior_prob_yes/norm),(posterior_prob_no/norm)

new_instance = {'Color': 'Blue', 'Type': 'SUV', 'Origin': 'Domestic'}
p_yes_given_new,p_no_given_new =calc_new_inst(new_instance['Color'],new_instance['Type'],new_instance['Origin'])
print (p_yes_given_new)
print (p_no_given_new)

new_instance = {'Color': 'Blue', 'Type': 'SUV', 'Origin': 'Domestic'}
p_yes_given_new_laplace,p_no_given_new_laplace =calc_laplace_new_inst(new_instance['Color'],new_instance['Type'],new_instance['Origin'])
print (p_yes_given_new_laplace)
print (p_no_given_new_laplace)

if p_yes_given_new > p_no_given_new:
    predicted_class = 'Yes'
else:
    predicted_class = 'No'

print(f"The predicted class for t he new instance is: {predicted_class}")

if p_yes_given_new_laplace > p_no_given_new_laplace:
    predicted_class_l = 'Yes'
else:
    predicted_class_l = 'No'

print(f"The predicted class using laplace smoothing for t he new instance is: {predicted_class_l}")

"""# **Part 2**

## **(a)**
"""

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
dataset = pd.read_csv(url)
# Split the dataset into features and labels
X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

dataset

# shuffled_data = dataset.sample(frac=1, random_state=42)  # Shuffle the entire dataset

# # Split the dataset into training and test sets
# split_percentage = 0.8  # 80% for training, 20% for testing
# split_index = int(len(shuffled_data) * split_percentage)

# X_train = shuffled_data.iloc[:split_index, :-1]
# y_train = shuffled_data.iloc[:split_index, -1]

# X_test = shuffled_data.iloc[split_index:, :-1]
# y_test = shuffled_data.iloc[split_index:, -1]

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
dataset = pd.read_csv(url)

X = dataset.iloc[:, :-1]
y = dataset.iloc[:, -1]

# Calculate the index to split the data
split_idx = int(0.8 * len(dataset))

# Split the data into training and test sets
X_train = X[:split_idx]
Y_train = y[:split_idx]
X_test = X[split_idx:]
Y_test = y[split_idx:]

gaussian_nb = GaussianNB().fit(X_train, Y_train)

multinomial_nb = MultinomialNB()
multinomial_nb.fit(X_train, Y_train)

#Make predictions on the test data using both classifiers
gaussian_pred = gaussian_nb.predict(X_test)
multinomial_pred = multinomial_nb.predict(X_test)

#Calculate the confusion matrix and accuracy scores for gaussian classifiers
gaussian_cm = confusion_matrix(Y_test, gaussian_pred)
gaussian_accuracy = accuracy_score(Y_test, gaussian_pred)
gaussian_nb.score(X_test, Y_test)
print("Gaussian Naive Bayes Accuracy:", gaussian_accuracy)
print("Gaussian Naive Bayes Confusion Matrix:")
print(gaussian_cm)
ConfusionMatrixDisplay.from_estimator(gaussian_nb , X_test, Y_test)

#Calculate the confusion matrix and accuracy scores for multinomial classifiers
multinomial_cm = confusion_matrix(Y_test, multinomial_pred)
multinomial_accuracy = accuracy_score(Y_test, multinomial_pred)
print("Multinomial Naive Bayes Accuracy:", multinomial_accuracy)
print("Multinomial Naive Bayes Confusion Matrix:")
print(multinomial_cm)
multinomial_cm = accuracy_score(Y_test, gaussian_pred)
multinomial_nb.score(X_test, Y_test)
ConfusionMatrixDisplay.from_estimator( multinomial_nb , X_test, Y_test)

"""## **(b) Use train test split function on input and output of the whole data and utilize 80% of samples as train and 20% of samples as test data**"""

x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2, random_state=42)

NGS = GaussianNB()
NGS.fit(x_train, y_train)

NGS.score(x_test, y_test)

print('\nConfusion Matrix:\n')

ConfusionMatrixDisplay.from_estimator(NGS, x_test, y_test)

clf = MultinomialNB(force_alpha=True)
clf.fit(x_train, y_train)

clf.score(x_test, y_test)

print('\nConfusion Matrix:\n')

ConfusionMatrixDisplay.from_estimator(clf, x_test, y_test)

"""## **(c) Use another Naive Bayes classifier of your choice**"""

brn = BernoulliNB(force_alpha=True)
brn.fit(x_train, y_train)

brn.score(x_test, y_test)

y_pred = brn.predict(X_test)
print(classification_report(y_test, y_pred))

print('\nConfusion Matrix:\n')

ConfusionMatrixDisplay.from_estimator(brn, x_test, y_test)

"""## **(d) split the data into four equal parts according to order**"""

# Calculate the index to split the data
split_idx1 = int(0.25 * len(x_train))
split_idx2 = int(0.5  * len(y_train))
split_idx3 = int(0.75 * len(x_train))

# Split the training data into 4 equal parts
subset_1_x = x_train[:split_idx1]
subset_1_y = y_train[:split_idx1]

subset_2_x = x_train[split_idx1:split_idx2]
subset_2_y = y_train[split_idx1:split_idx2]

subset_3_x = x_train[split_idx2:split_idx3]
subset_3_y = y_train[split_idx2:split_idx3]

subset_4_x = x_train[split_idx3:]
subset_4_y = y_train[split_idx3:]

gaussian_nb1 = GaussianNB().fit(subset_1_x, subset_1_y)

multinomial_nb1 = MultinomialNB()
multinomial_nb1.fit(subset_1_x, subset_1_y)

gaussian_nb1.score(x_test, y_test)

multinomial_nb1.score(x_test, y_test)

gaussian_nb2 = GaussianNB().fit(subset_2_x, subset_2_y)

multinomial_nb2 = MultinomialNB()
multinomial_nb2.fit(subset_2_x, subset_2_y)

gaussian_nb2.score(x_test, y_test)

multinomial_nb2.score(x_test, y_test)

gaussian_nb3 = GaussianNB().fit(subset_3_x, subset_3_y)

multinomial_nb3 = MultinomialNB()
multinomial_nb3.fit(subset_3_x, subset_3_y)

gaussian_nb3.score(x_test, y_test)

multinomial_nb2.score(x_test, y_test)

gaussian_nb4 = GaussianNB().fit(subset_4_x, subset_4_y)

multinomial_nb4 = MultinomialNB()
multinomial_nb4.fit(subset_4_x, subset_4_y)

gaussian_nb4.score(x_test, y_test)

multinomial_nb4.score(x_test, y_test)