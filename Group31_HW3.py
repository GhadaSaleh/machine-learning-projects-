# -*- coding: utf-8 -*-
"""ML_Assignment_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N1-4NAwsUovU30AjRlVSfVK4vTDwZSdZ
"""

import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from tensorflow.keras import layers
from sklearn.metrics import f1_score
from sklearn.decomposition import PCA
from tensorflow.keras.models import Model
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import f_classif
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Input, Dense
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score, confusion_matrix

np.random.seed(42)

"""# **Part 2:**
### **1) a**

"""

myData = pd.read_csv('/content/MCSDatasetNEXTCONLab.csv')
print(myData.head())

train_data = myData[myData['Day'].isin([0, 1, 2])]

X_train = train_data.iloc[:,:-1]
Y_train = train_data.iloc[:, -1]

X_train = X_train.drop(['ID', 'Day'], axis=1)
# X_train = X_train.drop('Day', axis=1)

test_data = myData[myData['Day']==3]

X_test = test_data.iloc[:,:-1]
Y_test = test_data.iloc[:, -1]

X_test = X_test.drop(['ID','Day'], axis=1)
# X_test = X_test.drop('Day', axis=1)

scaler = MinMaxScaler()
# scaler = StandardScaler()
scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.fit_transform(X_test)

#Knn classifier
neigh = KNN(n_neighbors=3)
neigh.fit(X_train, Y_train)

#confusion Matrix Knn classifier
ConfusionMatrixDisplay.from_estimator(neigh, X_train, Y_train)
ConfusionMatrixDisplay.from_estimator(neigh, X_test, Y_test)

#F1 score Knn classifier
y_pred = neigh.predict(X_test)
print(classification_report(Y_test, y_pred))

# f1_knn = f1_score(Y_test, y_pred, average='weighted')
f1_knn = f1_score(Y_test, y_pred)
print(f1_knn)

f_Knn_accuracy_score = accuracy_score(Y_test, y_pred)

#naive bayes classifier
gnb = GaussianNB()
y_pred_NB = gnb.fit(X_train, Y_train).predict(X_test)

#confusion Matrix & F1 score naive bayes classifier
f1_NB = f1_score(Y_test, y_pred_NB)
print(f1_NB)

#F1 score NB classifier
y_pred = gnb.predict(X_test)
print(classification_report(Y_test, y_pred))

#confusion Matrix NB classifier
ConfusionMatrixDisplay.from_estimator(gnb, X_train, Y_train)
ConfusionMatrixDisplay.from_estimator(gnb, X_test, Y_test)



def plot_tsne(X, y, class_names, xlabel, ylabel):
    # Compute t-SNE embedding
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    X_embedded = tsne.fit_transform(X)
    # Create a DataFrame with the embedding and class labels
    data = pd.DataFrame(X_embedded, columns=['x', 'y'])
    data['class'] = y
    # Set the color palette​
    palette = sns.color_palette('bright', n_colors=len(class_names))
    # Plot the t-SNE embedding colored by class​
    fig, ax = plt.subplots(figsize=(8, 8))
    for i, class_name in enumerate(class_names):
        mask = data['class'] == class_name
        ax.scatter(data.loc[mask, 'x'], data.loc[mask, 'y'],   label=class_name, s=50, alpha=0.8, c=palette[i])
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.legend(loc='upper right')
    # Show the plot​

    plt.show()

class_names = list(np.unique(Y_train))
# print(class_names)
plot_tsne(X_train, Y_train, class_names, 'x training data', 'y training data')

plot_tsne(X_test, Y_test, class_names, 'x testing data', 'y testing data')

unique_count_train = test_data['Ligitimacy'].value_counts()
unique_count_train

unique_count_test  = train_data['Ligitimacy'].value_counts()
unique_count_test

plot_tsne(scaled_X_train, Y_train, class_names, 'scaled x training data', 'y training data')
plot_tsne(scaled_X_test, Y_train, class_names, 'scaled x testing data', 'y testing data')



"""### **2) Dimentionality Reduction:**"""

def evaluate_dimensionality_reduction_PCA(X_train, X_test, y_train, y_test, classifier):
    f1_scores = []
    dimensions = range(1, X_train.shape[1] + 1)

    for n_components in dimensions:
        # Perform PCA dimensionality reduction
        pca = PCA(n_components=n_components, random_state=0)
        X_train_pca = pca.fit_transform(X_train)
        X_test_pca = pca.transform(X_test)

        # Train and test the classifier
        clf = classifier.fit(X_train_pca, y_train)
        y_pred = clf.predict(X_test_pca)

        # Calculate the F1 score
        f1 = f1_score(y_test, y_pred)
        f1_scores.append(f1)

    return dimensions, f1_scores

# Evaluate NB classifier with PCA
nb_dimensions, nb_f1_scores = evaluate_dimensionality_reduction_PCA(scaled_X_train, scaled_X_test, Y_train, Y_test, GaussianNB())

# Evaluate KNN classifier with PCA
knn_dimensions, knn_f1_scores = evaluate_dimensionality_reduction_PCA(scaled_X_train, scaled_X_test, Y_train, Y_test, KNN())

best_PCA_components_nb = nb_dimensions[np.argmax(nb_f1_scores)]
print('best PCA in NB is ',best_PCA_components_nb )
best_PCA_components_knn = knn_dimensions[np.argmax(knn_f1_scores)]
print('best PCA in KNN is ',best_PCA_components_knn )

# Create a figure with subplots
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot 1
axes[0].plot(nb_dimensions, nb_f1_scores, marker='o', linestyle='-', color='b', label='NB')
axes[0].axhline(y=f1_NB, color='g', linestyle='--', label='NB baseline')
axes[0].set_xlabel('Number of Components')
axes[0].set_ylabel('F1 Score')
axes[0].set_title('NB')
axes[0].legend()

# Plot 2
axes[1].plot(knn_dimensions, knn_f1_scores, marker='o', linestyle='-', color='r', label='KNN')
axes[1].axhline(y=f1_knn, color='g', linestyle='-', label='KNN baseline')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('F1 Score')
axes[1].set_title('KNN')
axes[1].legend()

plt.tight_layout()

plt.show()

plt.figure(figsize=(8, 6))
# Plot the baseline performances and PCA of NB
plt.axhline(y=f1_NB, color='g', linestyle='--', label='NB baseline')
plt.plot(nb_dimensions, nb_f1_scores, marker='o', linestyle='-', color='b', label='NB')

plt.xlabel('Number of Components')
plt.ylabel('F1 Score')
plt.title('Number of Components vs F1 Score')
plt.legend()

# plt.figure()
# Plot the baseline performances and PCA of KNN
plt.axhline(y=f1_knn, color='g', linestyle='-', label='KNN baseline')
plt.plot(knn_dimensions, knn_f1_scores, marker='o', linestyle='-', color='r', label='KNN')



# Add labels and title
plt.xlabel('Number of Components')
plt.ylabel('F1 Score')
plt.title('Number of Components vs F1 Score')
plt.legend()
plt.show()

"""**Auto Encoder**"""

input_dim = X_train.shape[1]
encoding_dim = range(1, input_dim + 1)

def autoencoder_model(encoding_dim):
    input_layer = Input(shape=(input_dim,))
    encoder_layer = Dense(encoding_dim, activation='relu')(input_layer)
    decoder_layer = Dense(input_dim, activation='sigmoid')(encoder_layer)
    autoencoder = Model(inputs=input_layer, outputs=decoder_layer)
    return autoencoder

def evaluate_dimensionality_reduction_AE(X_train, X_test, y_train, y_test, classifier):
    f1_scores = []

    for n_components in encoding_dim:
        # Create the AE model
        autoencoder = autoencoder_model(n_components)
        autoencoder.compile(optimizer='adam', loss='mean_squared_error')

        # Fit the AE model to the training data
        autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, verbose=0)

        # Get the reduced dimension representations
        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(index=1).output)
        X_train_encoded = encoder.predict(X_train)
        X_test_encoded = encoder.predict(X_test)

        # Train and test the classifier
        clf = classifier.fit(X_train_encoded, y_train)
        y_pred = clf.predict(X_test_encoded)

        # Calculate the F1 score
        f1 = f1_score(y_test, y_pred)
        f1_scores.append(f1)

    return encoding_dim, f1_scores

# Evaluate NB classifier with AE
nb_ae_dimensions, scaled_nb_ae_f1_scores = evaluate_dimensionality_reduction_AE(scaled_X_train, scaled_X_test, Y_train, Y_test, GaussianNB())

# Evaluate KNN classifier with AE
knn_ae_dimensions, scaled_knn_ae_f1_scores = evaluate_dimensionality_reduction_AE(scaled_X_train, scaled_X_test, Y_train, Y_test, KNN())

# Create a figure with subplots
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot 1
axes[0].plot(nb_ae_dimensions, scaled_nb_ae_f1_scores, marker='o', linestyle='-', color='b', label='NB with AE')
axes[0].axhline(y=f1_NB, color='g', linestyle='-', label='NB baseline')
axes[0].set_xlabel('Number of Components')
axes[0].set_ylabel('F1 Score')
axes[0].set_title('NB')
axes[0].legend()

# Plot 2
axes[1].plot(knn_ae_dimensions, scaled_knn_ae_f1_scores, marker='o', linestyle='-', color='r', label='KNN with AE')
axes[1].axhline(y=f1_knn, color='g', linestyle='-', label='KNN baseline')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('F1 Score')
axes[1].set_title('KNN')
axes[1].legend()

plt.tight_layout()

plt.show()

# Plot the F1 scores vs the number of components for NB with AE
plt.figure(figsize=(8, 6))
plt.plot(nb_ae_dimensions, scaled_nb_ae_f1_scores, marker='o', linestyle='-', color='b', label='NB with AE')


# Plot the F1 scores vs the number of components for KNN with AE
plt.plot(knn_ae_dimensions, scaled_knn_ae_f1_scores, marker='o', linestyle='-', color='r', label='KNN with AE')

# Plot the baseline performances
plt.axhline(y=f1_knn, color='g', linestyle='-', label='KNN baseline')
plt.axhline(y=f1_NB, color='g', linestyle='--', label='NB baseline')

plt.legend()
plt.show()

#get improved data and fi result according to the best no. of components obtained in PCA with KNN
pca = PCA(n_components= 3, random_state=0)
X_train_pca = pca.fit_transform(scaled_X_train)
X_test_pca = pca.transform(scaled_X_test)
clfknn = KNN().fit(X_train_pca, Y_train)
y_pred_improved = clfknn.predict(X_test_pca)
f1_baseline_improved_knn = f1_score(Y_test, y_pred_improved)
f1_baseline_improved_knn

#get improved data and fi result according to the best no. of components obtained in PCA with KNN
pca = PCA(n_components= 8, random_state=0)
X_train_pca = pca.fit_transform(scaled_X_train)
X_test_pca = pca.transform(scaled_X_test)
clfgnb = GaussianNB().fit(X_train_pca, Y_train)
y_pred_improved = clfgnb.predict(X_test_pca)
f1_baseline_improved_gnb = f1_score(Y_test, y_pred_improved)
f1_baseline_improved_gnb

plot_tsne(X_train_pca, Y_train, class_names, 'improved x training data', 'y training data')

plot_tsne(X_test_pca, Y_test, class_names, 'improved x testing data', 'y testing data')

"""### **3) Feature Selection**

1- Filtering method (Information gain)
"""

my_X = myData.drop('Ligitimacy', axis=1)
my_X = myData.drop('ID', axis=1)
my_X = myData.drop('Day', axis=1)
my_Y = myData['Ligitimacy']

# List to store f1 scores for each number of features
nb_f1_scores = []
knn_f1_scores = []

# Define the range of number of features to consider
num_features_range = range(1, my_X.shape[1] + 1)

for num_features in num_features_range:
    # Apply SelectKBest to select the top k features based on information gain
    selector = SelectKBest(score_func=f_classif, k=num_features)
    X_selected = selector.fit_transform(my_X, my_Y)

    # Split the dataset into training and testing sets
    X_trainy = X_selected[myData['Day'].isin([0, 1, 2])]
    X_testy = X_selected[myData['Day']==3]

    # Train the Naive Bayes classifier
    nb_classifier = GaussianNB()
    nb_classifier.fit(X_trainy, Y_train)
    nb_predictions = nb_classifier.predict(X_testy)
    nb_f1_scores.append(f1_score(Y_test, nb_predictions))

    # Train the KNN classifier
    knn_classifier = KNN()
    knn_classifier.fit(X_trainy, Y_train)
    knn_predictions = knn_classifier.predict(X_testy)
    knn_f1_scores.append(f1_score(Y_test, knn_predictions))

# Create a figure with subplots
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot 1
axes[0].plot(num_features_range, nb_f1_scores, marker='o', linestyle='-', color='b', label='Naive Bayes')
axes[0].axhline(y=f1_baseline_improved_gnb, color='g', linestyle='-', label='NB improved baseline')
axes[0].set_xlabel('Number of Components')
axes[0].set_ylabel('F1 Score')
axes[0].set_title('NB')
axes[0].legend()

# Plot 2
axes[1].plot(num_features_range, knn_f1_scores, marker='o', linestyle='-', color='r', label='KNN')
axes[1].axhline(y=f1_baseline_improved_knn, color='g', linestyle='-', label='improved KNN baseline')
axes[1].set_xlabel('Number of Components')
axes[1].set_ylabel('F1 Score')
axes[1].set_title('KNN')
axes[1].legend()

plt.tight_layout()

plt.show()

# Calculate the information gain of each feature in the training set
selector = SelectKBest(score_func=mutual_info_classif, k=3)
X_train_selected = selector.fit_transform(X_train, Y_train)

# Apply the same feature selection to the test set
X_test_selected = selector.transform(X_test)

plot_tsne(X_train_selected, Y_train, class_names,'filtered x testing data', 'y testing data')
plot_tsne(X_test_selected, Y_test, class_names,'filtered x testing data', 'y testing data')



"""2- wrapper method"""



My_dataset = pd.read_csv('MCSDatasetNEXTCONLab.csv')
My_train_data = My_dataset[My_dataset['Day'].isin([0, 1, 2])]
My_test_data = My_dataset[My_dataset['Day'] == 3]

# h=np.unique(My_test_data["Day"])

my_X_train = My_train_data.drop('Ligitimacy', axis=1)
my_X_train = my_X_train.drop('ID', axis=1)
my_X_train = my_X_train.drop('Day', axis=1)
my_Y_train = My_train_data['Ligitimacy']

my_X_test = My_test_data.drop('Ligitimacy', axis=1)
my_X_test = my_X_test.drop('ID', axis=1)
my_X_test = my_X_test.drop('Day', axis=1)
my_Y_test = My_test_data['Ligitimacy']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
my_X_train = scaler.fit_transform(my_X_train)
my_X_test = scaler.transform(my_X_test)

from sklearn.naive_bayes import GaussianNB
my_nb_model = GaussianNB()
my_nb_model.fit(my_X_train, my_Y_train)

# my_KNN_model = KNN()
# my_KNN_model.fit(my_X_train, my_Y_train)

from sklearn.metrics import accuracy_score
nb_predictions = my_nb_model.predict(my_X_test)
# knn_predictions = my_KNN_model.predict(my_X_test)
# my_nb_confusion_matrix = confusion_matrix(my_Y_test, nb_predictions)
f1_NB_baseline = f1_score(my_Y_test, nb_predictions, average='macro')
f1_NB_accuracy_score = accuracy_score(my_Y_test, nb_predictions)
# f1_Knn_accuracy_score = accuracy_score(my_Y_test, knn_predictions)
# print("Naive Bayes Classifier Confusion Matrix:")
# print(my_nb_confusion_matrix)
print("Naive Bayes Classifier F1 Score:", f1_NB_baseline)

from sklearn.metrics import accuracy_score
nb_predictions = my_nb_model.predict(my_X_test)
nb_accuracy = accuracy_score(my_Y_test, nb_predictions)
print("Naive Bayes Classifier Accuracy:", nb_accuracy)

input_dim = my_X_train.shape[1]
latent_dim = int(input_dim / 2)
dimensions_AE_nb = []
f1_scores_NB_AE = []

for n in range(1, (latent_dim)*2+1):
    input_layer = Input(shape=(input_dim,))
    encoder_layer = Dense(n, activation='relu')(input_layer)
    decoder_layer = Dense(input_dim, activation='sigmoid')(encoder_layer)
    autoencoder = Model(inputs=input_layer, outputs=decoder_layer)

    my_X_train_AE = np.copy(my_X_train)
    my_X_test_AE = np.copy(my_X_test)

    autoencoder.compile(optimizer='adam', loss='mean_squared_error')
    autoencoder.fit(my_X_train, my_X_train_AE, epochs=10, batch_size=32, verbose=1)


    encoder = models.Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)
    X_train_encoded = encoder.predict(my_X_train)
    X_test_encoded = encoder.predict(my_X_test)



    my_nb_model.fit(X_train_encoded,my_Y_train)
    y_pred_nb = my_nb_model.predict(X_test_encoded[:, :n])
    f1_nb = f1_score(my_Y_test, y_pred_nb)

    dimensions_AE_nb.append(n)
    f1_scores_NB_AE.append(f1_nb)


plt.plot(dimensions_AE_nb, f1_scores_NB_AE, label='NB')
plt.axhline(y=f1_NB_baseline, color='g', linestyle='-', label='NB baseline')

plt.xlabel('Number of Components (Dimensions)')
plt.ylabel('F1 Score')
plt.title('Autoencoder Dimensionality Reduction Performance')
plt.legend()
plt.show()




best_autoencoder_components = dimensions_AE_nb[np.argmax(f1_scores_NB_AE)]
# plot_tsne(my_X_train_AE, my_Y_train, class_names,"training AE Plot")

from imblearn.over_sampling import RandomOverSampler
oversampler = RandomOverSampler(random_state=42)
X_oversampled, y_oversampled = oversampler.fit_resample(my_X_train, my_Y_train)
h = my_Y_train.value_counts()
print(h)
h1 = y_oversampled.value_counts()
print(h1)

from sklearn.metrics import accuracy_score
def get_nb_f1_score(train_features, train_labels, test_features, test_labels):
    nb_classifier = GaussianNB()
    nb_classifier.fit(train_features, train_labels)
    predictions = nb_classifier.predict(test_features)
    f1 = accuracy_score(test_labels, predictions)

    return f1

from sklearn.neighbors import KNeighborsClassifier as KNN
def get_knn_f1_score(train_features, train_labels, test_features, test_labels):
    knn_classifier = KNN()
    knn_classifier.fit(train_features, train_labels)
    predictions = knn_classifier.predict(test_features)
    f1 = accuracy_score(test_labels, predictions)
    return f1

from sklearn.feature_selection import SelectKBest, mutual_info_classif

def select_features(train_features, train_labels, num_features):
    selector = SelectKBest(mutual_info_classif, k=num_features)
    selector.fit(train_features, train_labels)
    selected_features = selector.get_support(indices=True)
    return selected_features

def wrapping_method_nb(num_features, train_features, train_labels, test_features, test_labels):
    selected_features = select_features(train_features, train_labels, num_features)
    train_features_subset = train_features[:, selected_features]
    test_features_subset = test_features[:, selected_features]
    f1 = get_knn_f1_score(train_features_subset, train_labels, test_features_subset, test_labels)
    return f1

def wrapping_method_knn(num_features, train_features, train_labels, test_features, test_labels):
    selected_features = select_features(train_features, train_labels, num_features)
    train_features_subset = train_features[:, selected_features]
    test_features_subset = test_features[:, selected_features]
    f1 = get_nb_f1_score(train_features_subset, train_labels, test_features_subset, test_labels)
    return f1

num_features_list = [1,2,3,4,5,6,7,8,9,10]  # Example list of numbers of features to try
f1_scores = []
for num_features in num_features_list:
    f1 = wrapping_method_nb(num_features, my_X_train, my_Y_train, my_X_test, my_Y_test)
    f1_scores.append(f1)
plt.plot(num_features_list, f1_scores, marker='o')
plt.xlabel('Number of Features')
plt.ylabel('F1 Score')
plt.title('Number of Features vs. accuracy Score in Wrapping for Naive Bayes Classifier')
plt.axhline(y=f1_NB_accuracy_score, color='g', linestyle='-', label='NB baseline')
plt.show()

num_features_list = [1,2,3,4,5,6,7,8,9,10]  # Example list of numbers of features to try
f1_scores = []
for num_features in num_features_list:
    f1 = wrapping_method_knn(num_features, my_X_train, my_Y_train, my_X_test, my_Y_test)
    f1_scores.append(f1)
plt.plot(num_features_list, f1_scores, marker='o')
plt.xlabel('Number of Features')
plt.ylabel('F1 Score')
plt.axhline(y=f_Knn_accuracy_score, color='g', linestyle='-', label='KNN baseline')
plt.title('Number of Features vs. accuracy Score in Wrapping for Knn Classifier')
plt.show()

"""## **4) Kmeans Clustering Latitude and longitude features.**

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
!pip install MiniSom
from sklearn.preprocessing import MinMaxScaler
from minisom import MiniSom
# Load the dataset
dataset = pd.read_csv('MCSDatasetNEXTCONLab.csv')

dataset.head(5)

# Separate features and target variable
X = dataset[['Latitude', 'Longitude', 'Day', 'Hour', 'Minute', 'Duration', 'RemainingTime', 'Resources',
             'Coverage', 'OnPeakHours', 'GridNumber']]
y = dataset['Ligitimacy']

# Split the dataset into training and test datasets
X_train = X[X['Day'].isin([0, 1, 2])]
y_train = y[X['Day'].isin([0, 1, 2])]

X_test = X[X['Day'] == 3]
y_test = y[X['Day'] == 3]

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_train_new =X_train[:, [0, 1]].copy()

# Train the Naive Bayes Classifier
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Train the K-Nearest Neighbor Classifier
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(X_train, y_train)

# Define the number of clusters to evaluate
n_clusters = [8, 12, 16, 20, 32]

# Store the number of legitimate only members for each number of clusters
legitimate_members = []
nonlegitimate_members = []
final_legit =[]
# Perform clustering and calculate the number of legitimate only members
# Perform clustering and calculate the number of legitimate only members
for n in n_clusters:
    kmeans = KMeans(n_clusters=n, random_state=42)
    kmeans.fit(X_train_new)
    labels = kmeans.labels_
    unique_labels, counts = np.unique(labels[y_train == 1], return_counts=True)
    unique_labels2, counts2 = np.unique(labels[y_train == 0], return_counts=True)

    # Check if any labels are missing for legitimate members
    if len(unique_labels) < n:
        missing_labels = np.setdiff1d(np.arange(n), unique_labels)
        missing_counts = np.zeros_like(missing_labels)
        unique_labels = np.concatenate((unique_labels, missing_labels))
        counts = np.concatenate((counts, missing_counts))

    # Check if any labels are missing for non-legitimate members
    if len(unique_labels2) < n:
        missing_labels = np.setdiff1d(np.arange(n), unique_labels2)
        missing_counts = np.zeros_like(missing_labels)
        unique_labels2 = np.concatenate((unique_labels2, missing_labels))
        counts2 = np.concatenate((counts2, missing_counts))

    legitimate_members.append(counts)
    nonlegitimate_members.append(counts2)
    num_sum = 0

    for i in range(n):
        if (counts[i] / (counts[i] + counts2[i])) > 0.97:
            num_sum += counts[i]
    final_legit.append(num_sum)
    # print(unique_labels)
    # print(unique_labels2)
    # print(counts2)

    # for i in range(n):
    #     if (legitimate_members[i] / (legitimate_members[i] + nonlegitimate_members[i])) > 0.97:
    #         num_sum += legitimate_members[i]
    # final_legit.append(num_sum)


# Plot the number of clusters vs the total number of legitimate only members
plt.plot(n_clusters, final_legit, marker='o', linestyle='-', color='b')
plt.xlabel('Number of Clusters')

plt.ylabel('Number of Ligitimate Only Members')
plt.show()

# legitimate_members

X_train_new = X_train_new.astype(np.float64)
cluster_numbers = [8, 12, 16, 20, 32]

# Store the number of legitimate only members for each number of clusters
legitimate_only_members = []
nonlegitimate_members = []
legitimate_members = []
final_legit =[]

for n_clusters in cluster_numbers:
    som = MiniSom(n_clusters, 1, 2, sigma=0.5, learning_rate=0.5, random_seed=0)
    som.train(X_train_new, 100)

    labels = np.array([som.winner(x.astype(np.float64))[0] for x in X_train_new])
    unique_labels, counts = np.unique(labels[y_train == 1], return_counts=True)
    unique_labels2, counts2 = np.unique(labels[y_train == 0], return_counts=True)

    # Check if any labels are missing for legitimate members
    if len(unique_labels) < n:
        missing_labels = np.setdiff1d(np.arange(n), unique_labels)
        missing_counts = np.zeros_like(missing_labels)
        unique_labels = np.concatenate((unique_labels, missing_labels))
        counts = np.concatenate((counts, missing_counts))

    # Check if any labels are missing for non-legitimate members
    if len(unique_labels2) < n:
        missing_labels = np.setdiff1d(np.arange(n), unique_labels2)
        missing_counts = np.zeros_like(missing_labels)
        unique_labels2 = np.concatenate((unique_labels2, missing_labels))
        counts2 = np.concatenate((counts2, missing_counts))

    legitimate_members.append(counts)
    nonlegitimate_members.append(counts2)
    num_sum = 0

    for i in range(n):
        if (counts[i] / (counts[i] + counts2[i])) > 0.97:
            num_sum += counts[i]
    legitimate_only_members.append(num_sum)

#     unique_labels, counts = np.unique(labels, return_counts=True)
#     legitimate_clusters = counts[np.where(unique_labels != -1)]
#     max_legitimate_only_members = np.max(legitimate_clusters)
#     legitimate_only_members.append(max_legitimate_only_members)

plt.plot(cluster_numbers, legitimate_only_members, 'ro-')
plt.xlabel('Number of Clusters')
plt.ylabel('Total Number of Legitimate Only Members')
plt.title('SOFM Clustering')
plt.show()

from sklearn.cluster import DBSCAN


# Filter legitimate data points
X_legitimate = X_train[y_train == 1]

# Define the parameters to try
parameters = [
    {'midPoint': (0, 0), 'epsilon': 0.1},
    {'midPoint': (0, 0), 'epsilon': 0.2},
    {'midPoint': (0, 0), 'epsilon': 0.3},
    {'midPoint': (0, 0), 'epsilon': 0.4},
    {'midPoint': (0, 0), 'epsilon': 0.5}
]

# Store the number of legitimate only members for each parameter setting
legitimate_members = []

# Apply DBSCAN for each parameter setting
for param in parameters:
    dbscan = DBSCAN(eps=param['epsilon'], min_samples=5)
    dbscan.fit(X_legitimate)
    labels = dbscan.labels_
    unique_labels, counts = np.unique(labels, return_counts=True)
    legitimate_members.append(counts.max())

# Plot the number of clusters vs the total number of legitimate only members
n_clusters = [8, 12, 16, 20, 32]  # The desired number of clusters
plt.plot(n_clusters, legitimate_members, marker='o', linestyle='-', color='b')
plt.xlabel('Number of Clusters')
plt.ylabel('Number of Legitimate Only Members')
plt.title(' (DBSCAN)')
plt.show()